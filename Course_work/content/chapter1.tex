
% Содержание первой главы

\chapter{\MakeUppercase{Разведочный анализ данных с использованием PySpark}}\label{ch:first}

\section{Постановка задачи разведочного анализа}\vspace{\baselineskip}

Задачей данной главы является проведение разведочного анализа большого набора данных о поведении пользователей электронной коммерции с использованием возможностей фреймворка Apache Spark.
Основные задачи разведочного анализа заключаются в следующем:

\begin{itemize}
\item загрузка данных из распределённой файловой системы HDFS и формирование единого датафрейма;
\item исследование структуры, схемы и качества данных;
\item выявление пропусков, дубликатов и некорректных значений;
\item преобразование типов данных и создание производных признаков;
\item предварительная оценка распределений количественных признаков и анализа категориальных данных;
\item подготовка очищенного и структурированного набора данных для последующего применения алгоритмов машинного обучения.
\end{itemize}

Результатом данного этапа является построение целостного представления о данных и формирование корректной основы для дальнейших шагов анализа и моделирования.
\vspace{\baselineskip}

\section{Описание исходного датасета}\vspace{\baselineskip}

В работе используется датасет <<eCommerce behavior data from multi category store>>, доступный на платформе Kaggle \cite{kaggle2023dataset}. Набор данных состоит из одного датасета — 2019-Nov.csv. В дальнейшем его название изменится на dataset.csv.

Датафрейм включает следующие ключевые признаки (таблица \ref{tab:features}):

\begin{table}[H]
    \centering
    \begin{tabularx}{\textwidth}{|X|X|}
        \hline
        Признак & Описание \\
        \hline
        event\_time & Время, когда произошло событие (UTC). \\
        \hline
        event\_type & Вид события. \\
        \hline
        product\_id & Идентификатор продукта. \\
        \hline
        category\_id & Идентификатор категории продукта. \\
        \hline
        category\_code & Таксономия категории товара (кодовое название). \\
        \hline
        brand & Строка с названием бренда. \\
        \hline
        price & Цена продукта. \\
        \hline
        user\_id & Идентификатор пользователя. \\
        \hline
    \end{tabularx}
    \caption{Описание признаков датасета}
    \label{tab:features}
\end{table}

Совокупный объём данных составляет более 67 миллионов строк. Загруженные данные изначально имеют строковые типы и разнородные форматы идентификаторов. Чтение и демонстрация исходных данных производилось с помощью следующего кода:

\begin{code}
path = "hdfs://namenode:9000/user/dchel/dataset.csv"
df = (spark.read.format("csv")
      .option("header", "true")
      .load(path)
)
df.show()
\end{code}

Данный фрагмент показывает, что:

\begin{itemize}
\item исходные данные находятся в HDFS;
\item происходит чтение csv файла dataset.csv;
\item происходит вывод первых 20 строк датафрейма в консоль.
\end{itemize}

Полный вывод результата выполнения код представлен на рисунке \ref{fig:ExampleData1}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Content/Images/Analyze/ExampleData.png}
    \caption{Данные из датасета}
    \label{fig:ExampleData1}
\end{figure}

В ходе анализа полей датасета, с помощью команды \texttt{df.select(
"event\_type", "product\_id", "category\_id", "category\_code
", "brand", "price")} были выбраны следующие поля:  \texttt{event\_type},  \texttt{product\_id},  \texttt{category\_id},  \texttt{category\_code},  \texttt{brand}, \texttt{price} (см. рис. \ref{fig:ExampleDataSelect}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Content/Images/Analyze/ExampleDataAfterSelect.png}
    \caption{Данные из датасета после select}
    \label{fig:ExampleDataSelect}
\end{figure}

Структура данных была изучена с использованием команды \texttt{df.printSc
hema()}, что позволило определить типы полей и выявить их потенциальную неоднородность. Так, поля \texttt{event\_type}, \texttt{product\_id}, \texttt{category\_id}, \texttt{category\_code}, \texttt{brand}, \texttt{price} и текстовые поля загружаются как строки, что указывает на возможное наличие разнородных форматов данных. Структура представлена на рисунке (см. рис. \ref{fig:DataFrameScheme}).

\begin{figure}[htbp]
    \centering
    \includegraphics{Content/Images/Analyze/DataFrameScheme.png}
    \caption{Структура таблицы после загрузки данных}
    \label{fig:DataFrameScheme}
\end{figure}

Более детальное изучение содержимого выполнялось уже на этапе разведочного анализа при помощи выборочного просмотра записей df.show() (см. рис. \ref{fig:ExampleDataAfterShow}), анализа уникальных значений и регулярных выражений.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Content/Images/Analyze/ExampleDataAfterShow.png}
    \caption{Выборочный просмотр записей}
    \label{fig:ExampleDataAfterShow}
\end{figure}

Эти методы позволили установить, что:

\begin{itemize}
\item числовые идентификаторы различной длины (от 6 до 8 цифр);
\item только одно значение view во всех строках выборки;
\item множество значений NULL;
\item значительный разброс цен (от бюджетных товаров до премиальных);
\item числовые значения с двумя десятичными знаками.
\end{itemize}

Параметры Spark-сессии были настроены с учётом объёма данных \cite{karau2015spark, damji2020learning}: увеличены объёмы памяти драйвера и исполнителей. Это обеспечивает стабильную работу при чтении и трансформации больших датафреймов. На рисунке \ref{fig:SparkSession} показана конфигурация SparkSession.

В конфиге, указанном ниже, последние строчки указывают на то, что используется spark:

\begin{code}
def create_spark_configuration()-> SparkConf:
    user_name = "dchel"
    conf = SparkConf()
    conf.setAppName("Lab 1")
    conf.setMaster("local[*]")
    conf.set("spark.submit.deployMode", "client")
    conf.set("spark.executor.memory", "12g")
    conf.set("spark.executor.cores", "8")
    conf.set("spark.executor.instances", "2")
    conf.set("spark.driver.memory", "4g")
    conf.set("spark.driver.cores", "2")
    conf.set("spark.sql.catalog.spark_catalog.type",
    "hadoop")
    conf.set("spark.sql.catalog.spark_catalog.warehouse",
    f"hdfs:///user/{user_name}")
    conf.set("spark.sql.catalog.spark_catalog.io-impl",
    "org.apache.iceberg.hadoop.HadoopFileIO")
    return conf
\end{code}

\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{Content/Images/Analyze/SparkSession.png}
    \caption{Демонстрация работы сессии Spark}
    \label{fig:SparkSession}
\end{figure}

Также для работы с датасетом, он был предварительно загружен в hdfs. Обзор директории представлен на рисунке \ref{fig:DatasetInHadoop}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Content/Images/Analyze/DatasetInHadoop.png}
    \caption{Директория с данными для работы}
    \label{fig:DatasetInHadoop}
\end{figure}

\vspace{\baselineskip}

\section{Определение пропущенных значений и преобразование данных}\vspace{\baselineskip}

Анализ полноты данных показал наличие пропусков в ряде столбцов, преимущественно в числовых полях. Для оценки количества пропусков была использована служебная функция, выполняющая подсчёт NULL-значений:

\begin{code}
def count_nulls(data: DataFrame,
                column_name: str)-> None:
    null_counts = data.select(
    sum(col(column_name).isNull().cast("int"))
    ).collect()[0][0]

    not_null_counts = data.select(
        sum(col(column_name).isNotNull().cast("int"))
    ).collect()[0][0]

    print(f"Число колонок с NULL: {null_counts} "
          f"({100 * null_counts / (null_counts +
          not_null_counts):.2f}%)")
\end{code}

\vspace{\baselineskip}
Были применены следующие стратегии:

\begin{itemize}
\item удалены строки содержащие \texttt{NULL}-значения в столбцах \texttt{category\_co
de} и \texttt{brand} командой \texttt{data.dropna(subset=["category\_code
", "brand"])};
\item текстовый столбец \texttt{category\_id} был удален с помощью команды \texttt{df.drop("category\_id")}.
\end{itemize}

После очистки структура данных была расширена и дополнена новыми признаками. В частности, выполнено преобразование типов:

\begin{itemize}
\item численный перевод идентификаторов продуктов (\texttt{product\_id});
\item перевод кодов категории в массив строк (\texttt{category\_code});
\item численный перевод цены (\texttt{price}).
\end{itemize}

Дополнительно созданы следующие производные признаки:

\begin{itemize}
\item массив кодов категорий, полученный путём разбиения строки с использованием \texttt{split};
\item признак содержания вида продукта \texttt{contains\_appliances}, \texttt{contains\_computers}, \texttt{contains\_electronics}, \texttt{contains\_ki
tchen}, \texttt{contains\_smartphone};
\item булевый признак дороговизны продукта \texttt{is\_expensive};
\item булевый признак бюджетного продукта \texttt{is\_budget};
\item булевый признак среднебюджетного продукта \texttt{is\_mid\_range};
\item кол-во категорий, которые охватывают продукт \texttt{category\_count};
\item булевый признак просмотра продукта \texttt{is\_view};
\item булевый признак добавление продукта в корзину \texttt{is\_cart};
\item булевый признак покупки продукта \texttt{is\_purchase};
\item класс продукта \texttt{price\_range};
\item класс продукта в численном формате \texttt{price\_range\_numeric}.
\end{itemize}

Бинарные признаки создаются с помощью следующего скрипта:

\begin{code}
df = df.withColumn("is_expensive", when(col("price") > 200, 1)
    .otherwise(0))
df = df.withColumn("is_budget", when(col("price") < 50, 1)
    .otherwise(0))
df = df.withColumn("is_mid_range", when((col("price") >= 50) 
    & (col("price") <= 200), 1).otherwise(0))
df = df.withColumn("is_purchase", 
    when(col("event_type") == "purchase", 1).otherwise(0))
df = df.withColumn("is_view", 
    when(col("event_type") == "view", 1).otherwise(0))
df = df.withColumn("is_cart", 
    when(col("event_type") == "cart", 1).otherwise(0))
\end{code}

Количественные признаки создаются с помощью следующего скрипта:

\begin{code}
df = df.withColumn("category_count", 
                   col("contains_appliances").cast("int") + 
                   col("contains_computers").cast("int") + 
                   col("contains_electronics").cast("int") + 
                   col("contains_kitchen").cast("int") + 
                   col("contains_smartphone").cast("int"))
\end{code}

Категориальные признаки создаются с помощью следующего скрипта:

\begin{code}
df = df.withColumn("price_range", 
                   when(col("price") < 50, "budget")
                   .when(col("price") < 150, "affordable")
                   .when(col("price") < 300, "premium")
                   .otherwise("luxury"))
df = df.withColumn("price_range_numeric",
                   when(col("price_range") == "budget", 1)
                   .when(col("price_range") == "affordable",
                     2)
                   .when(col("price_range") == "premium", 3)
                   .otherwise(4))
\end{code}

Результаты продемонстрированы на рисунках \ref{fig:SchemeAfterProcessing1} и \ref{fig:ExampleDataAfterProcessing}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Content/Images/Analyze/SchemeAfterProcessing.png}
    \caption{Структура таблицы после обработки данных}
    \label{fig:SchemeAfterProcessing1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Content/Images/Analyze/ExampleDataAfterProcessing.png}
    \caption{Фрагмент данных после обработки}
    \label{fig:ExampleDataAfterProcessing}
\end{figure}

Все преобразования были объединены в функцию, позволяющую повторно применять трансформации к датафрейму. Скрипт вынесен в Приложение \ref{app:transform_data}.


\vspace{\baselineskip}

\section{Анализ распределений, выбросов и категориальных признаков}\vspace{\baselineskip}

Для количественного признака \texttt{price}, была построена гистограмма с использованием Spark и последующей визуализацией в библиотеке Matplotlib (см. Приложение \ref{app:hist}).

Фрагмент кода расчета данных для построения гистограммы распределения:

\begin{code}
min_value = data.selectExpr(f"min({column})").collect()[0][0]
max_value = data.selectExpr(f"max({column})").collect()[0][0]
bin_size = (max_value - min_value) / num_bins

data_with_bin = data.selectExpr("*", 
    f"floor(({column} - {min_value}) / {bin_size}) as bin"
).filter(f"bin < {num_bins}")
bin_counts = data_with_bin.groupBy("bin").count()
    .orderBy("bin")
bin_counts_list = bin_counts.collect()
bin_indices = []
bin_values = []
for row in bin_counts_list:
    bin_indices.append(row['bin'])
    bin_values.append(row['count'])
bin_centers = [min_value + (bin_idx + 0.5) * bin_size 
    for bin_idx in bin_indices]
\end{code}

Полученные результаты показывают, что cтоимость продуктов пользователей смещена в сторону невысоких значений (мода 125–150)  (см. рис. \ref{fig:PriceField}).

\begin{figure}[H]
    \centering
    \includegraphics[width=1.02\textwidth]{Content/Images/Analyze/PriceField.png}
    \caption{Гистограмма распределения для price}
    \label{fig:PriceField}
\end{figure}

На рисунках \ref{fig:PriceAnomaly1}, \ref{fig:PriceAnomaly2} продемонстрировано распределение аномальных значений у price. Фукнция построения и расчетов данных для ящика с усами вынесена в Приложение \ref{app:plot_boxplots}.

Вычисление квантилей и медианы для построения ящика с усами выполняется в следующем фрагменте:

\begin{code}
for column in columns:
    quantiles = data.approxQuantile(column, 
        [0.25, 0.5, 0.75], 0.01)
    q1, median, q3 = quantiles
\end{code}

Вычисление границ и межквартильного размаха для построения ящика с усами выполняется в следующем фрагменте:

\begin{code}
iqr = q3 - q1
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr
\end{code}

Ограничение усов минимальным и максимальным значениями в графике выполняется в следующем фрагменте:

\begin{code}
lower_bound = max(lower_bound, min_value)
upper_bound = min(upper_bound, max_value)
\end{code}

Вычисление среднеквадратичного отклоненения, среднего, минимального и максимального значений для вывода статистических характеристик:

\begin{code}
min_value = data.agg({column: "min"}).collect()[0][0]
mean_value = data.agg({column: "mean"}).collect()[0][0]
std_value = data.agg({column: "std"}).collect()[0][0]
max_value = data.agg({column: "max"}).collect()[0][0]
print(f"Минимальное значение:          {min_value:.2f}")
print(f"Среднее значение:              {mean_value:.2f}")
print(f"Среднеквадратичное отклонение: {std_value:.2f}")
print(f"Первый квартиль:               {q1:.2f}")
print(f"Медиана:                       {median:.2f}")
print(f"Третий квартиль:               {q3:.2f}")
print(f"Максимальное значение:         {max_value:.2f}")
\end{code}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{Content/Images/Analyze/PriceAnomaly1.png}
    \caption{Пример аномалий у price}
    \label{fig:PriceAnomaly1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Content/Images/Analyze/PriceAnomaly2.png}
    \caption{Расчетные значения у price}
    \label{fig:PriceAnomaly2}
\end{figure}

Для категориальных признаков \texttt{category\_code}, \texttt{brand} были проанализированы частоты встречаемости. Самым популярным значением для поля:
\begin{itemize}
\item \texttt{brand} стало samsung (рис. \ref{fig:BrandCategoriesHist});
\item \texttt{category\_code} стало electronics (рис. \ref{fig:CategoryCodeHist}).
\end{itemize} 

Расчет данных для построения графиков выполняется в следующем фрагменте кода:
\begin{code}
column_type = dict(data.dtypes)[column_name]
if column_type == 'array<string>':
    categories = (data
        .select(explode(col(column_name)).alias(column_name))
        .groupBy(column_name).count().orderBy("count", 
            ascending=False)
    )
else:
    categories = (data
        .groupBy(column_name).count().orderBy("count",
             ascending=False)
    )
total_categories = categories.count()
categories_list = categories.limit(top_n).collect()
category_names = []
category_counts = []
for row in categories_list:
    category_names.append(row[column_name])
    category_counts.append(row['count'])
\end{code}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Content/Images/Analyze/BrandCategoriesHist.png}
    \caption{Частоты для brand}
    \label{fig:BrandCategoriesHist}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Content/Images/Analyze/CategoryCodeHist.png}
    \caption{Частоты для category\_code}
    \label{fig:CategoryCodeHist}
\end{figure}

Проверка дубликатов показала, что некоторые товары имеют повторяющиеся записи — это связано с тем, что каждая запись соответствует отдельному пользовательскому действию. Такие дубликаты являются ожидаемыми и отражают структуру исходного набора данных. Поэтому для всего датасета была проведена дедубликация по \texttt{product\_id}, \texttt{event\_type} и \texttt{price} полям командой \texttt{df.dropDuplicates(["product\_id", "event\_type", "price"])}.

\vspace{\baselineskip}

\section{Выводы}\vspace{\baselineskip}

В ходе проведённого разведочного анализа был сформирован целостный и очищенный набор данных, готовый для применения алгоритмов машинного обучения. Основными результатами являются:

\begin{itemize}
\item выполнена загрузка данных из HDFS средствами Apache Spark;
\item исследована структура данных, выявлены и обработаны пропуски и аномалии;
\item проведён анализ распределений и выбросов в количественных характеристиках;
\item проведён анализ частот встречаемости значений в категориальных характеристиках;
\item сформированы новые признаки, повышающие информативность дан
ных;
\item выявлены особенности набора данных, связанные с дублированием записей по идентификатору продукта.
\end{itemize}